********************************************************************************************
********************************************************************************************
          On MASTER VM Commands
********************************************************************************************
********************************************************************************************

[root@master ~]# wget https://download.schedmd.com/slurm/slurm-22.05.7.tar.bz2
[root@master ~]# yum install mariadb-server mariadb-devel -y
[root@master ~]# yum install epel-release -y
[root@master ~]# yum install munge munge-libs munge-devel -y
[root@master ~]# yum install rpm-build -y
[root@master ~]# yum install python3 -y
[root@master ~]# yum install pam-devel
[root@master ~]# yum  install perl-ExtUtils*
[root@master ~]# yum install gcc
[root@master ~]# rpmbuild -ta slurm-22.05.7.tar.bz2
[root@master ~]# /usr/sbin/create-munge-key -r
[root@master ~]# ll /etc/munge/
[root@master ~]# chown -R munge:munge /etc/munge
[root@master ~]# ll /etc/munge/
[root@master ~]# chmod 400 /etc/munge/munge.key
-----------------------------------------------------------------
     First Install Munge on Nodes then Copy munge key
-----------------------------------------------------------------
[root@master ~]# scp /etc/munge/munge.key root@node1:/etc/munge/
[root@master ~]# cd /root/rpmbuild/RPMS/x86_64/
[root@master x86_64]# yum install slurm*
[root@master x86_64]# cp /etc/slurm/slurm.conf.example /etc/slurm/slurm.conf
[root@master x86_64]# cp /etc/slurm/cgroup.conf.example /etc/slurm/cgroup.conf
[root@master x86_64]# vi /etc/slurm/slurm.conf
----------------------------------------------
ClusterName=hpcsa
SlurmctldHost=master

SlurmdSpoolDir=/var/share/slurm/d
StateSaveLocation=/var/share/slurm/ctld
----------------------------------------------
 Change these Line 7 save file
----------------------------------------------
[root@master x86_64]# touch /var/log/slurmctld.log
-----------------------------------------------------------------
     First Install Slurm on Nodes then Copy Slurm conf file
-----------------------------------------------------------------
[root@master x86_64]# export SLURMUSER=1002
[root@master x86_64]# groupadd -g $SLURMUSER slurm
[root@master x86_64]# useradd  -m -c "SLURM workload manager" -d /var/lib/slurm -u $SLURMUSER -g slurm  -s /bin/bash slurm
[root@master x86_64]# chown -R slurm:slurm /var/share/slurm
[root@master x86_64]# mkdir -p /var/share/slurm/ctld
[root@master x86_64]# chown -R slurm:slurm /etc/slurm
[root@master ~]# scp  /etc/slurm/slurm.conf root@node1:/etc/slurm/
[root@master ~]# scp  /etc/slurm/cgroup.conf root@node1:/etc/slurm/
--------------------------------------------------------------------------------------------------
Run this command on Client machine & Copy Output Line
[root@node1 x86_64]# slurmd -C
--------------------------------------------------------------------------------------------------
[root@master x86_64]# vi /etc/slurm/slurm.conf
--------------------------------------------------------------------------------------------------
# COMPUTE NODES
NodeName=node1 CPUs=1 Boards=1 SocketsPerBoard=1 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=1819
NodeName=node2 CPUs=1 Boards=1 SocketsPerBoard=1 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=1819
--------------------------------------------------------------------------------------------------
  Add this line and Save
--------------------------------------------------------------------------------------------------
[root@node1 x86_64]# chown -R slurm:slurm /var/share/slurm
[root@master ~]# scp  /etc/slurm/slurm.conf root@node1:/etc/slurm/
[root@master ~]# scp  /etc/slurm/cgroup.conf root@node1:/etc/slurm/
[root@master ~]# systemctl start slurmctld
[root@master ~]# systemctl enable slurmctld
[root@master ~]# systemctl status slurmctld
[root@master ~]# systemctl start munge
[root@master ~]# systemctl enable munge
[root@master ~]# systemctl status munge
[root@master ~]# scontrol update nodename=node2 state=idle    [Node up command into Idle state]

=================================================================================================
   Slurm Accounting
=================================================================================================
[root@master ~]# cp /etc/slurm/slurmdbd.conf.example /etc/slurm/slurmdbd.conf
[root@master ~]# vi /etc/slurm/slurmdbd.conf
---------------------------------------------
# Database info
StorageType=accounting_storage/mysql
StorageHost=localhost
#StoragePort=1234
#StoragePass=password
StorageUser=slurm
StorageLoc=slurm_acct_db
---------------------------------------------
 save file
---------------------------------------------
[root@master ~]# chown slurm:slurm /etc/slurm/slurmdbd.conf
[root@master ~]# mkdir /var/log/slurm
[root@master ~]# touch /var/log/slurm/slurmdbd.log
[root@master ~]# systemctl start slurmdbd
[root@master ~]# systemctl enable slurmdbd
[root@master ~]# systemctl status slurmdbd
[root@master ~]# systemctl start mariadb
[root@master ~]# systemctl enable mariadb
[root@master ~]# systemctl status mariadb
[root@master ~]# vi /etc/slurm/slurm.conf
----------------------------------------------------
# LOGGING AND ACCOUNTING
AccountingStorageHost=localhost
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageUser=slurm
----------------------------------------------------
  save file
----------------------------------------------------
[root@master ~]# systemctl restart slurmctld
[root@master ~]# systemctl restart slurmdbd
[root@master ~]# systemctl status slurmdbd
[root@master ~]# systemctl enable slurmdbd
[root@master ~]# systemctl status slurmdbd

---------------------------------------------------------------------------------------------
 Accounting Commands
---------------------------------------------------------------------------------------------
[root@master ~]# mysql
mysql> create user 'slurm'@'localhost';
MariaDB [(none)]> grant all on slurm_acct_db.* TO 'slurm'@'localhost';
MariaDB [(none)]> show databases;
exit
[root@master ~]# sacctmgr
sacctmgr: show cluster
sacctmgr: show user
sacctmgr: add cluster slurmclustre
sacctmgr: add account slurm_account
sacctmgr: add user sneha account=slurm_account
sacctmgr: show qos                                        # show all qos      
sacctmgr: show user
sacctmgr: show assoc format=cluster,user,qos
sacctmgr: create  qos DEMO maxwall=2-00:00:00              # for create qos
sacctmgr: modify user root set qos+=sample                 # for create qos
sacctmgr: show assoc format=cluster,user,qos               # show all details
---------------------------------------------------------------------------------------------
***************   Create  User & User with Priority   ***************************
---------------------------------------------------------------------------------------------
[root@master ~]# sacctmgr
sacctmgr: create  qos DEMO maxwall=2-00:00:00              # Create QOS
sacctmgr: create qos hpcsa priority=11000                  # Create QOS with Priority

----------------------------------------------------------------------------------------------
            ******************  User Reservation  *************** 
----------------------------------------------------------------------------------------------
[root@master ~]# adduser shweta
[root@master ~]# passwd shweta
[root@master ~]# scp /etc/passwd root@node1:/etc/passwd    
[root@master ~]# scp /etc/shadow root@node1:/etc/shadow   
[root@master ~]# scp /etc/group root@node1:/etc/group    
[root@master ~]# scp /etc/passwd root@node2:/etc/passwd   
[root@master ~]# scp /etc/group root@node2:/etc/group    
[root@master ~]# scp /etc/shadow root@node2:/etc/shadow
[root@master ~]# cat /etc/passwd
-----------------------------------------------------------
Copy User ID (eg:- 1003)
Eg:->> shweta:x:1003:1003::/home/shweta:/bin/bash
------------------------------------------------------------
[root@master ~]# scontrol create reservationname=shweta start=now duration=1 \
> user=1003 partition=small tres=cpu=1,node=1
[root@master ~]# scontrol show res                                          #Show Reservation List
---------------------------------------------------------------------------------------------------
   **********   Create Reservation with Specific Date & Time   *******************
 ---------------------------------------------------------------------------------------------------
[root@master ~]# scontrol create reservation starttime=2023-02-02T21:10:30 \
> duration=120 user=root flags=maint,ignore_jobs nodes=ALL
---------------------------------------------------------------------------------------------------

===================================================================================================
        *************  Jobs Releted Basic Commands  ***************
===================================================================================================
[root@master ~]# sinfo
[root@master ~]# srun -N1 /bin/hostname
[root@master ~]# srun -N1 --pty /bin/bash
[root@master ~]# squeue                                       #show Partition
---------------------------------------------------------------------------------------------------
                                   Add partition in slurm
----------------------------------------------------------------------------------------------------
[root@master ~]# vi /etc/slurm/slurm.conf
-----------------------------------------------------------------------
PartitionName=small Nodes=node1 Default=YES MaxTime=INFINITE State=UP
------------------------------------------------------------------------
 Add this line in the last line of file slurm.conf
------------------------------------------------------------------------
[root@master ~]# systemctl restart slurmctld
[root@master ~]# sinfo                                     #show partition
----------------------------------------------------------------------------------------------------
[root@master ~]# scancel 12                                   [eg:- JobID-12, for delete job id]
[root@master ~]# scontrol show node node1                     [eg:- node1 is node]
[root@master ~]# scontrol show part                           [show all partition name & details]
[root@master ~]# scontrol show part medium                    [eg:- medium is partition name]


**********************************************************************************************************
**********************************************************************************************************
                       On NODE Command
**********************************************************************************************************
**********************************************************************************************************

[root@node1 ~]# yum install epel-release -y
[root@node1 ~]# yum install munge munge-libs munge-devel -y
[root@node1 ~]# mkdir -p /var/share/slurm/d
[root@node1 ~]# touch /var/log/slurmd.log
[root@node1 ~]# chown -R munge:munge /etc/munge
[root@node1 ~]# ll /etc/munge
[root@node1 ~]# chmod 400 /etc/munge/munge.key
[root@node1 ~]# cd /root/rpmbuild/RPMS/x86_64/
[root@node1 x86_64]# yum install slurm*
[root@node1 x86_64]# export SLURMUSER=1002
[root@node1 x86_64]# groupadd -g $SLURMUSER slurm
[root@node1 x86_64]# useradd  -m -c "SLURM workload manager" -d /var/lib/slurm -u $SLURMUSER -g slurm  -s /bin/bash slurm
[root@node1 x86_64]# chown -R slurm:slurm /var/share/slurm
[root@node1 x86_64]# slurmd -C
[root@node1 ~]# chown -R munge:munge /etc/munge
[root@node1 ~]# chmod 400 /etc/munge/munge.key
[root@node1 ~]# systemctl start slurmd
[root@node1 ~]# systemctl enable slurmd
[root@node1 ~]# systemctl status slurmd
[root@node1 ~]# systemctl start munge
[root@node1 ~]# systemctl enable munge
[root@node1 ~]# systemctl status munge

===================================================================================================
                   *************  JOB SCRIPT SAMPLE FILE    *************
===================================================================================================
[root@node1 ~]# sinfo
[root@node1 ~]# vi jobscript.sh
-------------------------------------------------------------------------------------------
#!/bin/bash
# Job name:
#SBATCH --job-name=test
#
# Account:
##SBATCH --account=account_name
#
# Partition:
##SBATCH --partition=partition_name
#
# Request one node:
#SBATCH --nodes=1
#
# Specify one task:
##SBATCH --ntasks-per-node=1
#
## Number of processors for single task needed for use case (example):
##SBATCH --cpus-per-task=4
#
# Wall clock limit:
#SBATCH --time=00:01:30
#
## Command(s) to run (example):
#export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
/bin/hostname
sleep 120
------------------------------------------------------------------------------------------------
  ********* JOB SUBMIT COMMAND **********
[root@node1 ~]# sbatch jobscript.sh
[root@node1 ~]# scontrol show job 25           #25 is Job ID
