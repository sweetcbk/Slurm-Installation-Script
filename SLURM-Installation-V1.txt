=======================================================================================
 SLURM Installation Process On [ OS: Centos7 ]
=======================================================================================
 Create 3 Virtual Machine with NAT mode, HDD-50GB, RAM-2GB
 Name:- 1st VM- Master, 2nd VM- Node1 & 3rd VM- Node2
=======================================================================================


====================================
  On Master Virtual Machine
====================================

[root@localhost ~]# hostnamectl set-hostname master
[root@localhost ~]# systemctl stop firewalld
[root@localhost ~]# systemctl disable firewalld
[root@localhost ~]# vim /etc/selinux/config
                    ------------------------
                         selinux=disabled
                    ------------------------
                          ADD THIS LINE 
                    ------------------------
[root@localhost ~]# vi /etc/hosts
                    -------------------------------
                     192.168.235.136 master
			   192.168.235.137 node1
			   192.168.235.138 node2
                    -------------------------------
[root@localhost ~]# ping node1
[root@localhost ~]# ping node2
[root@localhost ~]# reboot
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
--------------------
Chrony Installation
--------------------
[root@master ~]# yum -y install chrony
[root@master ~]# vi /etc/chrony.conf
---------------------------------------------------------------------------
# Use public servers from the pool.ntp.org project.
# Please consider joining the pool (http://www.pool.ntp.org/join.html).
server 1.centos.pool.ntp.org iburst
server 2.centos.pool.ntp.org iburst
server 3.centos.pool.ntp.org iburst

# Record the rate at which the system clock gains/losses time.
driftfile /var/lib/chrony/drift

# Allow the system clock to be stepped in the first three updates
# if its offset is larger than 1 second.
makestep 1.0 3

# Enable kernel synchronization of the real-time clock (RTC).
rtcsync


# Increase the minimum number of selectable sources required to adjust
# the system clock.
minsources 2

# Allow NTP client access from local network.

allow 192.168.235.0/24

# Serve time even if not synchronized to a time source.
local stratum 10

# Specify directory for log files.
logdir /var/log/chrony
------------------------------------------------------------------------------
 Remove all line & paste this lines and save file
-------------------------------------------------------------------------------
[root@master ~]# systemctl start chronyd
[root@master ~]# systemctl enable chronyd
[root@master ~]# systemctl status chronyd
[root@master ~]# chronyc sources

===============================================================================
--------------------
  NFS Installation
--------------------
[root@master]# ssh-keygen
               Press 3 time Enter

[root@master]#ssh-copy-id root@client
[root@master]# ssh client 
               Login & exit for client user

---------------------------------------------------------------------

-------
Step=2
-------

[root@master]# mkdir home1
[root@master]# yum install nfs-utils
[root@master]# systemctl stop firewalld
[root@master]# setenforce 0
[root@master]# systemctl start nfs-server.service
[root@master]# systemctl status nfs-server.service
[root@master]# vi /etc/exports
               Add this line in the file
               ----------------------------------------
                /root/home1/ *(rw,sync,no_root_squash)
               ----------------------------------------
                Save file

[root@master ~]# systemctl restart nfs-server.service 
[root@master]# exportfs -arv

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

----------------------------------------------------------------------------------------
[VM Browser]# Download:- https://download.schedmd.com/slurm/slurm-22.05.7.tar.bz2
[root@master ~]# yum install mariadb-server mariadb-devel -y
[root@master ~]# yum install epel-release -y
[root@master ~]# yum install munge munge-libs munge-devel -y
[root@master ~]# yum install rpm-build
[root@master ~]# yum install readline-devel
[root@master ~]# yum install python3 -y
[root@master ~]# yum install pam-devel
[root@master ~]# yum  install perl-ExtUtils*
[root@master ~]# yum install gcc
[root@master ~]# cp /root/Downloads/slurm-22.05.7.tar.bz2 /root/home1/
[root@master ~]# rpmbuild -ta /root/home1/slurm-22.05.7.tar.bz2
[root@master ~]# /usr/sbin/create-munge-key -r
[root@master ~]# ll /etc/munge/
[root@master ~]# chown -R munge:munge /etc/munge
[root@master ~]# ll /etc/munge/
[root@master ~]# scp /etc/munge/munge.key root@node1:/etc/munge/
[root@master ~]# scp /etc/munge/munge.key root@node2:/etc/munge/

[root@master ~]# scp /etc/slurm/cgroup.conf node1:/etc/slurm/
[root@master ~]# scp /etc/slurm/cgroup.conf node2:/etc/slurm/

[root@master ~]# chmod 400 /etc/munge/munge.key
[root@master ~]# cd /root/rpmbuild/RPMS/x86_64/
[root@master x86_64]# yum install slurm*
[root@master x86_64]# cp -ri /root/rpmbuild/RPMS/x86_64/ /root/home1/
[root@master x86_64]#
export SLURMUSER=992
groupadd -g $SLURMUSER slurm
useradd  -m -c "SLURM workload manager" -d /var/lib/slurm -u $SLURMUSER -g slurm  -s /bin/bash slurm

[root@master x86_64]# cp /etc/slurm/slurm.conf.example /etc/slurm/slurm.conf
[root@master x86_64]# mkdir -p /var/share/slurm/ctld
[root@master x86_64]# chown -R slurm:slurm /var/share/slurm
[root@master x86_64]# vi /etc/slurm/slurm.conf
----------------------------------------------
ClusterName=hpcsa
SlurmctldHost=master

SlurmdSpoolDir=/var/share/slurm/d
StateSaveLocation=/var/share/slurm/ctld
----------------------------------------------
 Change these Line 7 save file
----------------------------------------------

[root@master x86_64]# touch /var/log/slurmctld.log
--------------------------------------------------------------------------------------------------
Run this command on Client machine & Copy Output Line
[root@node1 x86_64]# slurmd -C
--------------------------------------------------------------------------------------------------
[root@master x86_64]# vi /etc/slurm/slurm.conf
--------------------------------------------------------------------------------------------------
# COMPUTE NODES
NodeName=node1 CPUs=1 Boards=1 SocketsPerBoard=1 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=1819
NodeName=node2 CPUs=1 Boards=1 SocketsPerBoard=1 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=1819
--------------------------------------------------------------------------------------------------
  Add this line and Save
--------------------------------------------------------------------------------------------------
[root@master ~]# systemctl start slurmctld
[root@master ~]# systemctl status slurmctld
[root@master ~]# systemctl start munge
[root@master ~]# systemctl status munge
[root@master ~]# scontrol update nodename=node2 state=idle    [Node up command into Idle state]

===================================================================================================
        ***** Run these command after complete slurm installation ****
===================================================================================================
[root@master ~]# sinfo
[root@master ~]# srun -N1 /bin/hostname
[root@master ~]# srun -N1 --pty /bin/bash
[root@master ~]# squeue
[root@master ~]# scancel 12                                   [eg:- JobID-12, for delete job id]
[root@master ~]# scontrol show node node1                     [eg:- node1 is node]
[root@master ~]# scontrol show part                           [show all partition name & details]
[root@master ~]# scontrol show part medium                    [eg:- medium is partition name]




---------------------------------------------------------------------------------------

====================================
  On Node-1 Virtual Machine
====================================

[root@localhost ~]# hostnamectl set-hostname node1
[root@localhost ~]# systemctl stop firewalld
[root@localhost ~]# systemctl disable firewalld
[root@localhost ~]# vim /etc/selinux/config
                    ------------------------
                         selinux=disabled
                    ------------------------
                          ADD THIS LINE 
                    ------------------------
[root@localhost ~]# vi /etc/hosts
                    -------------------------------
                     192.168.235.136 master
			   192.168.235.137 node1
			   192.168.235.138 node2
                    -------------------------------
[root@localhost ~]# ping master
[root@localhost ~]# ping node2
[root@localhost ~]# reboot


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
--------------------
Chrony Installation
--------------------

[root@master ~]# yum -y install chrony
[root@master ~]# vi /etc/chrony.conf
---------------------------------------------------------------------------------
# Use public servers from the pool.ntp.org project.
# Please consider joining the pool (http://www.pool.ntp.org/join.html).
server 192.168.235.161 iburst
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst

# Record the rate at which the system clock gains/losses time.
driftfile /var/lib/chrony/drift

# Allow the system clock to be stepped in the first three updates
# if its offset is larger than 1 second.
makestep 1.0 3

# Enable kernel synchronization of the real-time clock (RTC).
rtcsync


# Allow NTP client access from local network.
#allow 192.168.0.0/16

# Serve time even if not synchronized to a time source.
local stratum 10


# Specify directory for log files.
logdir /var/log/chrony
---------------------------------------------------------------------------------
Remove all line & paste this lines and save file
---------------------------------------------------------------------------------

[root@master ~]# systemctl start chronyd
[root@master ~]# systemctl enable chronyd
[root@master ~]# systemctl status chronyd
[root@master ~]# systemctl restart chronyd
[root@master ~]# ntpdate -q 192.168.235.161
[root@master ~]# ntpdate -u 192.168.235.161

=================================================================================
--------------------
  NFS Installation
--------------------
[root@client]# mkdir home1
[root@client]# yum install nfs-utils
[root@client]# systemctl stop firewalld
[root@client]# setenforce 0
[root@client]# showmount -e master
[root@client]# mount -t nfs master:/root/home1 /root/home1
[root@client]# df -Th

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

--------------------------------------------------------------------------------------
[root@master ~]# yum install epel-release -y
[root@master ~]# yum install munge munge-libs munge-devel -y

[root@node1 ~]# mkdir -p /var/share/slurm/d
[root@node1 ~]# chown -R slurm:slurm /var/share/slurm
[root@node1 ~]# touch /var/log/slurmd.log
[root@node1 ~]# chown -R munge:munge /etc/munge
[root@node1 ~]# ll /etc/munge
[root@master ~]# chmod 400 /etc/munge/munge.key
[root@node1 ~]# cd /root/home1/x86_64/
[root@node1 x86_64]# yum install slurm*
[root@master x86_64]#
export SLURMUSER=992
groupadd -g $SLURMUSER slurm
useradd  -m -c "SLURM workload manager" -d /var/lib/slurm -u $SLURMUSER -g slurm  -s /bin/bash slurm
[root@node1 x86_64]# slurmd -C
[root@node1 ~]# systemctl start slurmd
[root@node1 ~]# systemctl status slurmd
[root@node1 ~]# systemctl start munge
[root@node1 ~]# systemctl status munge

===================================================================================================
        ***** Run these command after complete slurm installation ****
===================================================================================================
[root@master ~]# sinfo
[root@node1 ~]# vi jobscript.sh
--------------------------------------------------
#!/bin/bash
# Job name:
#SBATCH --job-name=test
#
# Account:
##SBATCH --account=account_name
#
# Partition:
##SBATCH --partition=partition_name
#
# Request one node:
#SBATCH --nodes=1
#
# Specify one task:
##SBATCH --ntasks-per-node=1
#
## Number of processors for single task needed for use case (example):
##SBATCH --cpus-per-task=4
#
# Wall clock limit:
#SBATCH --time=00:01:30
#
## Command(s) to run (example):
#export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
/bin/hostname
sleep 120
--------------------------------------------------
[root@node1 ~]# sbatch jobscript.sh
[root@node1 ~]# scontrol show job 25

----------------------------------------------------------------------------------------------------

====================================
  On Node-2 Virtual Machine
====================================

[root@localhost ~]# hostnamectl set-hostname node2
[root@localhost ~]# systemctl stop firewalld
[root@localhost ~]# systemctl disable firewalld
[root@localhost ~]# vim /etc/selinux/config
                    ------------------------
                         selinux=disabled
                    ------------------------
                          ADD THIS LINE 
                    ------------------------
[root@localhost ~]# vi /etc/hosts
                    -------------------------------
                     192.168.235.136 master
			   192.168.235.137 node1
			   192.168.235.138 node2
                    -------------------------------
[root@localhost ~]# ping master
[root@localhost ~]# ping node1
[root@localhost ~]# reboot

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
--------------------
Chrony Installation
--------------------

[root@master ~]# yum -y install chrony
[root@master ~]# vi /etc/chrony.conf
---------------------------------------------------------------------------------
# Use public servers from the pool.ntp.org project.
# Please consider joining the pool (http://www.pool.ntp.org/join.html).
server 192.168.235.161 iburst
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst

# Record the rate at which the system clock gains/losses time.
driftfile /var/lib/chrony/drift

# Allow the system clock to be stepped in the first three updates
# if its offset is larger than 1 second.
makestep 1.0 3

# Enable kernel synchronization of the real-time clock (RTC).
rtcsync


# Allow NTP client access from local network.
#allow 192.168.0.0/16

# Serve time even if not synchronized to a time source.
local stratum 10


# Specify directory for log files.
logdir /var/log/chrony
---------------------------------------------------------------------------------
Remove all line & paste this lines and save file
---------------------------------------------------------------------------------

[root@master ~]# systemctl start chronyd
[root@master ~]# systemctl enable chronyd
[root@master ~]# systemctl status chronyd
[root@master ~]# systemctl restart chronyd
[root@master ~]# ntpdate -q 192.168.235.161
[root@master ~]# ntpdate -u 192.168.235.161

=================================================================================
--------------------
  NFS Installation
--------------------
[root@client]# mkdir home1
[root@client]# yum install nfs-utils
[root@client]# systemctl stop firewalld
[root@client]# setenforce 0
[root@client]# showmount -e master
[root@client]# mount -t nfs master:/root/home1 /root/home1
[root@client]# df -Th

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

------------------------------------------------------------------------------------
[root@master ~]# yum install epel-release -y
[root@master ~]# yum install munge munge-libs munge-devel -y
[root@node1 ~]# mkdir -p /var/share/slurm/d
[root@node1 ~]# chown -R slurm:slurm /var/share/slurm
[root@node1 ~]# touch /var/log/slurmd.log
[root@node1 ~]# chown -R munge:munge /etc/munge
[root@node1 ~]# ll /etc/munge
[root@master ~]# chmod 400 /etc/munge/munge.key
[root@node1 ~]# cd /root/home1/x86_64/
[root@node1 x86_64]# yum install slurm*
[root@master x86_64]#
export SLURMUSER=992
groupadd -g $SLURMUSER slurm
useradd  -m -c "SLURM workload manager" -d /var/lib/slurm -u $SLURMUSER -g slurm  -s /bin/bash slurm

[root@node1 x86_64]# slurmd -C
[root@node1 ~]# systemctl start slurmd
[root@node1 ~]# systemctl status slurmd
[root@node1 ~]# systemctl start munge
[root@node1 ~]# systemctl status munge

===================================================================================================
        ***** Run these command after complete slurm installation ****
===================================================================================================
[root@master ~]# sinfo

===========================================================================================
 All Detail Commands Reference:- https://slurm.schedmd.com/quickstart.html
===========================================================================================
